{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/N0zUCi0.png)"
      ],
      "metadata": {
        "id": "NyCh08KVQD9K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyp0-i4dHEHa"
      },
      "source": [
        "# LangChain LLM Interfacing Guide\n",
        "\n",
        "This notebook demonstrates how to connect to different Large Language Models (LLMs) using LangChain. We'll explore two main approaches:\n",
        "\n",
        "1. **`init_chat_model`** - Universal model initializer (recommended for flexibility)\n",
        "2. **Provider-specific interfaces** - Direct class imports like `ChatOpenAI`, `ChatGoogleGenerativeAI`, `ChatGroq`\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How to use `init_chat_model` for unified LLM initialization\n",
        "- How to use provider-specific chat interfaces\n",
        "- Simple prompting with `.invoke()`\n",
        "- Streaming responses\n",
        "\n",
        "## Models Covered\n",
        "\n",
        "- **OpenAI**: GPT-4.1-mini\n",
        "- **Google**: Gemini 2.5 Flash\n",
        "- **Groq**: OpenAI GPT OSS 120B (Open Source)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OEEdjuNHEHg"
      },
      "source": [
        "---\n",
        "\n",
        "## Installation\n",
        "\n",
        "First, install all required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "512yCS76HEHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7eee58-1965-4cc4-8ce6-647d9701fdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "!pip install -q langchain==1.2.8\n",
        "!pip install -q langchain-openai==1.1.7\n",
        "!pip install -q langchain-google-genai==4.2.0\n",
        "!pip install -q langchain-groq==1.1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9pzd-GxHEHi"
      },
      "source": [
        "---\n",
        "\n",
        "## Setup Environment Variables\n",
        "\n",
        "Set up your API keys securely. You'll need to obtain:\n",
        "\n",
        "- **OpenAI API Key**: Get from [OpenAI Platform](https://platform.openai.com/api-keys)\n",
        "- **Google API Key**: Get from [Google AI Studio](https://aistudio.google.com/app/apikey)\n",
        "- **Groq API Key**: Get from [Groq Console](https://console.groq.com/keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q34TXUW-HEHj",
        "outputId": "f9f3e717-bb6e-42e4-ee50-0286aa48b473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Enter your Google API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Enter your Groq API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Set up OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Set up Google API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "# Set up Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your Groq API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBfYoxHyHEHk"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 1: Using `init_chat_model`\n",
        "\n",
        "The `init_chat_model` function is a **universal model initializer** introduced in LangChain. It provides a consistent interface to initialize any supported chat model without needing to remember different import paths or class names.\n",
        "\n",
        "## Advantages of `init_chat_model`:\n",
        "\n",
        "- ‚úÖ **Single import** for all providers\n",
        "- ‚úÖ **Consistent API** across different LLMs\n",
        "- ‚úÖ **Easy model switching** - change one line to switch providers\n",
        "- ‚úÖ **Runtime configuration** - can make models configurable at runtime\n",
        "\n",
        "## Syntax\n",
        "\n",
        "```python\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Option 1: With explicit model_provider\n",
        "model = init_chat_model(\n",
        "    \"model-name\",\n",
        "    model_provider=\"provider-name\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Option 2: Auto-infer provider (works for common models)\n",
        "model = init_chat_model(\"gpt-4o\", temperature=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akc_yo50HEHl"
      },
      "source": [
        "## Example 1: OpenAI GPT-4.1-mini with `init_chat_model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM-arXMwHEHl",
        "outputId": "7b775185-3d68-4711-aef7-563d2d6efecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPT-4.1-mini initialized successfully using init_chat_model\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize GPT-4.1-mini using init_chat_model\n",
        "# We explicitly specify model_provider for clarity\n",
        "gpt_mini = init_chat_model(\n",
        "    \"gpt-4.1-mini\",\n",
        "    model_provider=\"openai\",  # Explicitly set provider\n",
        "    temperature=0  # Deterministic responses\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GPT-4.1-mini initialized successfully using init_chat_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4--1Js2HEHm"
      },
      "source": [
        "### Simple Prompt with `.invoke()`\n",
        "\n",
        "The `.invoke()` method sends a prompt to the LLM and returns the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-R8SZcOHEHm",
        "outputId": "11a5d7fe-4438-419f-e495-a0da879d79c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Explain what LangChain is 3 bullet points.\n",
            "\n",
            "Response:\n",
            "content='- LangChain is a framework designed to simplify the development of applications that use large language models (LLMs) by providing tools to manage prompts, chains, and integrations.  \\n- It enables the creation of complex workflows by connecting LLMs with external data sources, APIs, and other computational resources.  \\n- LangChain supports building applications like chatbots, question-answering systems, and document analysis tools with enhanced context management and memory capabilities.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 17, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_75546bd1a7', 'id': 'chatcmpl-D9DDwDSRBmEaDhNaIm6QqudaMbEWF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c5d14-905a-7853-9510-a936d2361dd5-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 17, 'output_tokens': 88, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Simple prompt example\n",
        "prompt = \"Explain what LangChain is 3 bullet points.\"\n",
        "\n",
        "# Invoke the model\n",
        "response = gpt_mini.invoke(prompt)\n",
        "\n",
        "# Display the response\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRU03cqcRqIT",
        "outputId": "fc6ed7bb-3502-431c-c7b0-6788ac23d8e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='- LangChain is a framework designed to simplify the development of applications that use large language models (LLMs) by providing tools to manage prompts, chains, and integrations.  \\n- It enables the creation of complex workflows by connecting LLMs with external data sources, APIs, and other computational resources.  \\n- LangChain supports building applications like chatbots, question-answering systems, and document analysis tools with enhanced context management and memory capabilities.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 17, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_75546bd1a7', 'id': 'chatcmpl-D9DDwDSRBmEaDhNaIm6QqudaMbEWF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c5d14-905a-7853-9510-a936d2361dd5-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 17, 'output_tokens': 88, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_U2c5tqRrgT",
        "outputId": "82e8f177-1be1-4966-8805-a65feac348aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- LangChain is a framework designed to simplify the development of applications that use large language models (LLMs) by providing tools to manage prompts, chains, and integrations.  \n",
            "- It enables the creation of complex workflows by connecting LLMs with external data sources, APIs, and other computational resources.  \n",
            "- LangChain supports building applications like chatbots, question-answering systems, and document analysis tools with enhanced context management and memory capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.usage_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZq3ooaSRtl1",
        "outputId": "bd82c929-609a-458b-889e-cfc09679016e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_tokens': 17, 'output_tokens': 88, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndJkb49dHEHm"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Using Provider-Specific Chat Interfaces\n",
        "\n",
        "While `init_chat_model` is convenient, you can also use **provider-specific chat classes**. These give you more direct control and access to provider-specific features.\n",
        "\n",
        "## When to use provider-specific interfaces:\n",
        "\n",
        "- Need access to **provider-specific parameters**\n",
        "- Want **type hints and autocomplete** for that provider\n",
        "- Building a production app with a **fixed provider**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t_5UWLuHEHn"
      },
      "source": [
        "## Example 2: OpenAI GPT-4.1-mini with `ChatOpenAI`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IpHflTTHEHn",
        "outputId": "181f5925-148c-49d8-e742-c045f25757ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPT-4.1-mini initialized successfully using ChatOpenAI\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize GPT-4.1-mini using the ChatOpenAI class\n",
        "# This is the traditional approach - importing the provider-specific class\n",
        "chat_openai = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",  # Specify the model name\n",
        "    temperature=0,  # Controls randomness (0 = deterministic, 1 = creative)\n",
        "    max_tokens=None,  # Maximum tokens in response (None = no limit)\n",
        "    timeout=None,  # Request timeout in seconds\n",
        "    max_retries=2,  # Number of retries on failure\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GPT-4.1-mini initialized successfully using ChatOpenAI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eI27FpbHEHn"
      },
      "source": [
        "### Simple Prompt with `.invoke()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQXzvQNrHEHo",
        "outputId": "08a34e06-3b17-4aa5-f89d-17df602f4c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the three main benefits of using LLMs?\n",
            "\n",
            "Response:\n",
            "content='The three main benefits of using Large Language Models (LLMs) are:\\n\\n1. **Natural Language Understanding and Generation:** LLMs can comprehend and produce human-like text, enabling more intuitive and effective communication between humans and machines.\\n\\n2. **Versatility Across Tasks:** They can perform a wide range of language-related tasks‚Äîsuch as translation, summarization, question answering, and content creation‚Äîwithout needing task-specific training.\\n\\n3. **Efficiency and Scalability:** LLMs can automate complex language tasks at scale, improving productivity and enabling applications that require processing large volumes of text quickly and accurately.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 19, 'total_tokens': 139, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_75546bd1a7', 'id': 'chatcmpl-D9DFDbKyPJBlkQUNxUFtcqJ0Wt61i', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c5d15-c8d5-79a0-bd91-a215852221de-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 19, 'output_tokens': 120, 'total_tokens': 139, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Simple prompt example\n",
        "prompt = \"What are the three main benefits of using LLMs?\"\n",
        "\n",
        "# Invoke the model\n",
        "response = chat_openai.invoke(prompt)\n",
        "\n",
        "# Display the response\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JwmcmHPSVAW",
        "outputId": "f79a9ea6-a0d7-4e1d-8543-08ce4fa93985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The three main benefits of using Large Language Models (LLMs) are:\\n\\n1. **Natural Language Understanding and Generation:** LLMs can comprehend and produce human-like text, enabling more intuitive and effective communication between humans and machines.\\n\\n2. **Versatility Across Tasks:** They can perform a wide range of language-related tasks‚Äîsuch as translation, summarization, question answering, and content creation‚Äîwithout needing task-specific training.\\n\\n3. **Efficiency and Scalability:** LLMs can automate complex language tasks at scale, improving productivity and enabling applications that require processing large volumes of text quickly and accurately.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 19, 'total_tokens': 139, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_75546bd1a7', 'id': 'chatcmpl-D9DFDbKyPJBlkQUNxUFtcqJ0Wt61i', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c5d15-c8d5-79a0-bd91-a215852221de-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 19, 'output_tokens': 120, 'total_tokens': 139, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFEPg5VgSWhR",
        "outputId": "f63bb5af-cc25-42b5-f8db-93d73a39d07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three main benefits of using Large Language Models (LLMs) are:\n",
            "\n",
            "1. **Natural Language Understanding and Generation:** LLMs can comprehend and produce human-like text, enabling more intuitive and effective communication between humans and machines.\n",
            "\n",
            "2. **Versatility Across Tasks:** They can perform a wide range of language-related tasks‚Äîsuch as translation, summarization, question answering, and content creation‚Äîwithout needing task-specific training.\n",
            "\n",
            "3. **Efficiency and Scalability:** LLMs can automate complex language tasks at scale, improving productivity and enabling applications that require processing large volumes of text quickly and accurately.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "vKQE6goHSZ3n",
        "outputId": "2d0eb0b9-3db2-4fb5-9153-a9673c2c28a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The three main benefits of using Large Language Models (LLMs) are:\n\n1. **Natural Language Understanding and Generation:** LLMs can comprehend and produce human-like text, enabling more intuitive and effective communication between humans and machines.\n\n2. **Versatility Across Tasks:** They can perform a wide range of language-related tasks‚Äîsuch as translation, summarization, question answering, and content creation‚Äîwithout needing task-specific training.\n\n3. **Efficiency and Scalability:** LLMs can automate complex language tasks at scale, improving productivity and enabling applications that require processing large volumes of text quickly and accurately."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia-af5KlHEHo"
      },
      "source": [
        "---\n",
        "\n",
        "## Example 3: Google Gemini 2.5 Flash Preview with `ChatGoogleGenerativeAI`\n",
        "\n",
        "Google's Gemini models are powerful multimodal LLMs. The 2.5 Flash Preview offers fast inference with strong capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FBSdRRLHEHo",
        "outputId": "5e66d5bc-7ab1-42e2-f797-de1a86416515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini 2.5 Flash initialized successfully\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize Gemini 2.5 Flash Preview\n",
        "# Model ID format: gemini-2.5-flash-preview-04-17\n",
        "# Note: Preview models may have different features than stable releases\n",
        "chat_gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Gemini 2.5 Flash initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_6xPjrlHEHp"
      },
      "source": [
        "### Simple Prompt with `.invoke()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CO9KkDDHEHp",
        "outputId": "2a197608-eb82-40b4-e46b-5b90bb4f17ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Explain the concept of prompt engineering in 3 bullets.\n",
            "\n",
            "Response:\n",
            "content=\"Here are 3 bullets explaining prompt engineering:\\n\\n*   **Crafting Effective Inputs:** Prompt engineering is the discipline of designing, refining, and optimizing the text inputs (prompts) given to AI models (especially large language models) to elicit specific, accurate, and desired outputs.\\n*   **Guiding AI Behavior:** It involves understanding how AI models process information and using various techniques‚Äîsuch as providing context, examples, constraints, or specific instructions‚Äîto steer the model's generation process towards a particular goal or format.\\n*   **Maximizing AI Utility:** The ultimate aim is to enhance the performance, reliability, and relevance of AI systems by minimizing ambiguity, reducing errors, and ensuring the AI consistently delivers high-quality, useful responses for a given task or application.\" additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019c5d16-ae10-76e2-9296-4e223a08d259-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 12, 'output_tokens': 980, 'total_tokens': 992, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 822}}\n"
          ]
        }
      ],
      "source": [
        "# Simple prompt example\n",
        "prompt = \"Explain the concept of prompt engineering in 3 bullets.\"\n",
        "\n",
        "# Invoke the model\n",
        "response = chat_gemini.invoke(prompt)\n",
        "\n",
        "# Display the response\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riCsq_vKT_h6",
        "outputId": "79ccd76d-b71d-4dad-9b13-fe889dfea5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Here are 3 bullets explaining prompt engineering:\\n\\n*   **Crafting Effective Inputs:** Prompt engineering is the discipline of designing, refining, and optimizing the text inputs (prompts) given to AI models (especially large language models) to elicit specific, accurate, and desired outputs.\\n*   **Guiding AI Behavior:** It involves understanding how AI models process information and using various techniques‚Äîsuch as providing context, examples, constraints, or specific instructions‚Äîto steer the model's generation process towards a particular goal or format.\\n*   **Maximizing AI Utility:** The ultimate aim is to enhance the performance, reliability, and relevance of AI systems by minimizing ambiguity, reducing errors, and ensuring the AI consistently delivers high-quality, useful responses for a given task or application.\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c5d16-ae10-76e2-9296-4e223a08d259-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 12, 'output_tokens': 980, 'total_tokens': 992, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 822}})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BFs3TyYUBFC",
        "outputId": "76ba614b-20d1-48d9-962b-9eba533752ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 bullets explaining prompt engineering:\n",
            "\n",
            "*   **Crafting Effective Inputs:** Prompt engineering is the discipline of designing, refining, and optimizing the text inputs (prompts) given to AI models (especially large language models) to elicit specific, accurate, and desired outputs.\n",
            "*   **Guiding AI Behavior:** It involves understanding how AI models process information and using various techniques‚Äîsuch as providing context, examples, constraints, or specific instructions‚Äîto steer the model's generation process towards a particular goal or format.\n",
            "*   **Maximizing AI Utility:** The ultimate aim is to enhance the performance, reliability, and relevance of AI systems by minimizing ambiguity, reducing errors, and ensuring the AI consistently delivers high-quality, useful responses for a given task or application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ATMtx3CUDWT",
        "outputId": "b1c1f022-b142-40a1-b6aa-74147056ca2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 bullets explaining prompt engineering:\n",
            "\n",
            "*   **Crafting Effective Inputs:** Prompt engineering is the discipline of designing, refining, and optimizing the text inputs (prompts) given to AI models (especially large language models) to elicit specific, accurate, and desired outputs.\n",
            "*   **Guiding AI Behavior:** It involves understanding how AI models process information and using various techniques‚Äîsuch as providing context, examples, constraints, or specific instructions‚Äîto steer the model's generation process towards a particular goal or format.\n",
            "*   **Maximizing AI Utility:** The ultimate aim is to enhance the performance, reliability, and relevance of AI systems by minimizing ambiguity, reducing errors, and ensuring the AI consistently delivers high-quality, useful responses for a given task or application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ChnZxc0sUHmd",
        "outputId": "92265ca0-93cd-4018-9c02-7e01858ddbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here are 3 bullets explaining prompt engineering:\n\n*   **Crafting Effective Inputs:** Prompt engineering is the discipline of designing, refining, and optimizing the text inputs (prompts) given to AI models (especially large language models) to elicit specific, accurate, and desired outputs.\n*   **Guiding AI Behavior:** It involves understanding how AI models process information and using various techniques‚Äîsuch as providing context, examples, constraints, or specific instructions‚Äîto steer the model's generation process towards a particular goal or format.\n*   **Maximizing AI Utility:** The ultimate aim is to enhance the performance, reliability, and relevance of AI systems by minimizing ambiguity, reducing errors, and ensuring the AI consistently delivers high-quality, useful responses for a given task or application."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfQo9KuKHEHp"
      },
      "source": [
        "---\n",
        "\n",
        "## Example 4: Groq with OpenAI GPT OSS 120B using `ChatGroq`\n",
        "\n",
        "Groq provides **ultra-fast inference** for open-source models. The OpenAI GPT OSS 120B model is a powerful open-source LLM available through Groq's LPU inference technology."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGV3TJH-HEHp",
        "outputId": "5f179ce2-c046-4f5c-b59e-32965cbe0ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI GPT OSS 120B (Groq) initialized successfully\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize GPT OSS 120B on Groq\n",
        "# Groq offers extremely fast inference speeds with their LPU architecture\n",
        "chat_groq = ChatGroq(\n",
        "    model=\"openai/gpt-oss-120b\",  # OpenAI GPT OSS 120B LLM\n",
        "    temperature=0,  # Deterministic responses\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ OpenAI GPT OSS 120B (Groq) initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNmwsS9vHEHq"
      },
      "source": [
        "### Simple Prompt with `.invoke()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aW5sgVxHEHq",
        "outputId": "d099df67-ba24-45c9-9001-808ca0ae6b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What makes open-source LLMs valuable for developers?\n",
            "\n",
            "Response:\n",
            "content='### Why Open‚ÄëSource LLMs Are a Big Deal for Developers\\n\\n| Benefit | What It Means for You | Typical Use‚ÄëCases |\\n|---------|----------------------|-------------------|\\n| **Full Transparency** | You can inspect the model architecture, training data (or at least the data‚Äëpipeline), and inference code. No hidden ‚Äúblack‚Äëbox‚Äù tricks. | Debugging unexpected outputs, compliance audits, academic research. |\\n| **Customizability** | Fine‚Äëtune, prune, quantize, or even modify the tokenizer to fit a niche domain (legal, medical, gaming, etc.). | Building domain‚Äëspecific assistants, code‚Äëcompletion tools, or chatbots that respect company terminology. |\\n| **Cost Control** | No per‚Äëtoken API fees; you only pay for the compute you run (or even run on‚Äëpremise on spare GPUs). | Scaling internal tools, batch processing large corpora, or running offline on edge devices. |\\n| **Data Privacy & Security** | All inference happens inside your own environment, so sensitive prompts never leave your network. | Handling PHI, financial data, or any proprietary information. |\\n| **Rapid Innovation Cycle** | Community contributions (new layers, LoRA adapters, efficient kernels) appear weeks after a research paper is released. | Staying on the cutting edge without waiting for a commercial vendor to add the feature. |\\n| **Vendor Independence** | No lock‚Äëin to a single cloud provider; you can move the model between on‚Äëprem, private cloud, or any public cloud that supports the runtime. | Multi‚Äëcloud strategies, disaster‚Äërecovery, or cost‚Äëoptimizing workloads. |\\n| **Ecosystem & Tooling** | Rich libraries (ü§ó\\u202fTransformers, vLLM, DeepSpeed, Axolotl, OpenAI‚Äëcompatible APIs) make training, serving, and monitoring straightforward. | End‚Äëto‚Äëend pipelines: data ingestion ‚Üí fine‚Äëtuning ‚Üí deployment ‚Üí observability. |\\n| **Community Support** | Forums, Discord/Slack channels, GitHub issues, and shared checkpoints accelerate problem‚Äësolving. | Getting help with GPU memory errors, prompt engineering, or model conversion. |\\n| **Licensing Flexibility** | Many models use permissive licenses (Apache\\u202f2.0, MIT, CC‚ÄëBY‚Äë4.0) that allow commercial use, redistribution, or modification. | Embedding the model in a SaaS product, bundling with hardware, or releasing a derivative model. |\\n| **Educational Value** | Hands‚Äëon exposure to state‚Äëof‚Äëthe‚Äëart architectures (Transformer variants, Mixture‚Äëof‚ÄëExperts, Retrieval‚ÄëAugmented Generation). | Training new hires, university courses, or personal skill development. |\\n\\n---\\n\\n#### Concrete Examples\\n\\n| Open‚ÄëSource Model | License | Typical Strengths | Example Projects |\\n|-------------------|---------|-------------------|------------------|\\n| **LLaMA‚Äë2** (Meta) | Meta‚ÄëLLAMA‚Äë2‚ÄëCommunity (non‚Äëcommercial) / LLaMA‚Äë2‚ÄëChat (commercial) | Strong general‚Äëpurpose performance, good instruction following. | Internal Q&A bots, code‚Äëreview assistants. |\\n| **Mistral‚Äë7B** | Apache\\u202f2.0 | Efficient 7‚ÄëB model with competitive zero‚Äëshot results. | Low‚Äëlatency inference on a single GPU, edge deployments. |\\n| **Phi‚Äë3** (Microsoft) | MIT | Tiny (2.7‚ÄëB) but high‚Äëquality, optimized for CPU/ONNX. | Desktop assistants, mobile apps, or serverless functions. |\\n| **Gemma** (Google) | Apache\\u202f2.0 | Balanced trade‚Äëoff between size (2‚ÄëB/7‚ÄëB) and instruction following. | Rapid prototyping of chat interfaces. |\\n| **OpenChatKit** | MIT | End‚Äëto‚Äëend chat framework with UI, retrieval, and moderation tools. | Building a customer‚Äësupport portal with minimal code. |\\n\\n---\\n\\n#### How to Get Started Quickly\\n\\n1. **Pick a Model & Runtime**  \\n   ```bash\\n   # Example: pull Mistral‚Äë7B with vLLM (fast serving)\\n   pip install vllm\\n   vllm serve mistralai/Mistral-7B-Instruct-v0.1 --port 8000\\n   ```\\n\\n2. **Wrap It in an OpenAI‚Äëcompatible API (optional)**  \\n   ```bash\\n   # Using `vllm`\\'s built‚Äëin OpenAI‚Äëcompatible endpoint\\n   curl http://localhost:8000/v1/chat/completions \\\\\\n        -H \"Content-Type: application/json\" \\\\\\n        -d \\'{\"model\":\"mistral\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain quantum tunneling in 2 sentences\"}]}\\'\\n   ```\\n\\n3. **Fine‚ÄëTune (if needed)**  \\n   - Use **PEFT** (LoRA) for cheap adaptation: `pip install peft transformers accelerate`.  \\n   - Example script: `python finetune_lora.py --model mistralai/Mistral-7B-Instruct-v0.1 --train_file data.jsonl`.\\n\\n4. **Deploy**  \\n   - **Docker**: `docker run -p 8000:8000 ghcr.io/vllm/vllm:latest --model mistralai/Mistral-7B-Instruct-v0.1`.  \\n   - **Kubernetes**: Helm chart `vllm` or `text-generation-inference` for autoscaling.\\n\\n---\\n\\n#### TL;DR\\n\\nOpen‚Äësource LLMs give developers **control, cost‚Äëefficiency, privacy, and a fast path to innovation**‚Äîall backed by a vibrant community and permissive licensing. They let you turn a generic language model into a **tailored, production‚Äëready service** without being locked into a proprietary API. Whether you‚Äôre building a chatbot, a code‚Äëassistant, or a data‚Äëanalysis pipeline, the open‚Äësource stack provides the building blocks to iterate quickly and own the entire stack.' additional_kwargs={'reasoning_content': 'The user asks: \"What makes open-source LLMs valuable for developers?\" Need to answer. Should be concise, informative. Possibly list benefits: transparency, customization, cost, community, data privacy, etc. Provide examples. No disallowed content. So answer.'} response_metadata={'token_usage': {'completion_tokens': 1315, 'prompt_tokens': 82, 'total_tokens': 1397, 'completion_time': 2.823389299, 'completion_tokens_details': {'reasoning_tokens': 55}, 'prompt_time': 0.003364365, 'prompt_tokens_details': None, 'queue_time': 0.004681638, 'total_time': 2.826753664}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_0708ac49a5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c5d17-573b-7b23-bd36-fbbcddcfbbd7-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 82, 'output_tokens': 1315, 'total_tokens': 1397, 'output_token_details': {'reasoning': 55}}\n"
          ]
        }
      ],
      "source": [
        "# Simple prompt example\n",
        "prompt = \"What makes open-source LLMs valuable for developers?\"\n",
        "\n",
        "# Invoke the model\n",
        "response = chat_groq.invoke(prompt)\n",
        "\n",
        "# Display the response\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qiqeSKzBVHif",
        "outputId": "dd2c5253-5faf-46b2-f769-889ad2024461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Why Open‚ÄëSource LLMs Are a Big Deal for Developers\n\n| Benefit | What It Means for You | Typical Use‚ÄëCases |\n|---------|----------------------|-------------------|\n| **Full Transparency** | You can inspect the model architecture, training data (or at least the data‚Äëpipeline), and inference code. No hidden ‚Äúblack‚Äëbox‚Äù tricks. | Debugging unexpected outputs, compliance audits, academic research. |\n| **Customizability** | Fine‚Äëtune, prune, quantize, or even modify the tokenizer to fit a niche domain (legal, medical, gaming, etc.). | Building domain‚Äëspecific assistants, code‚Äëcompletion tools, or chatbots that respect company terminology. |\n| **Cost Control** | No per‚Äëtoken API fees; you only pay for the compute you run (or even run on‚Äëpremise on spare GPUs). | Scaling internal tools, batch processing large corpora, or running offline on edge devices. |\n| **Data Privacy & Security** | All inference happens inside your own environment, so sensitive prompts never leave your network. | Handling PHI, financial data, or any proprietary information. |\n| **Rapid Innovation Cycle** | Community contributions (new layers, LoRA adapters, efficient kernels) appear weeks after a research paper is released. | Staying on the cutting edge without waiting for a commercial vendor to add the feature. |\n| **Vendor Independence** | No lock‚Äëin to a single cloud provider; you can move the model between on‚Äëprem, private cloud, or any public cloud that supports the runtime. | Multi‚Äëcloud strategies, disaster‚Äërecovery, or cost‚Äëoptimizing workloads. |\n| **Ecosystem & Tooling** | Rich libraries (ü§ó‚ÄØTransformers, vLLM, DeepSpeed, Axolotl, OpenAI‚Äëcompatible APIs) make training, serving, and monitoring straightforward. | End‚Äëto‚Äëend pipelines: data ingestion ‚Üí fine‚Äëtuning ‚Üí deployment ‚Üí observability. |\n| **Community Support** | Forums, Discord/Slack channels, GitHub issues, and shared checkpoints accelerate problem‚Äësolving. | Getting help with GPU memory errors, prompt engineering, or model conversion. |\n| **Licensing Flexibility** | Many models use permissive licenses (Apache‚ÄØ2.0, MIT, CC‚ÄëBY‚Äë4.0) that allow commercial use, redistribution, or modification. | Embedding the model in a SaaS product, bundling with hardware, or releasing a derivative model. |\n| **Educational Value** | Hands‚Äëon exposure to state‚Äëof‚Äëthe‚Äëart architectures (Transformer variants, Mixture‚Äëof‚ÄëExperts, Retrieval‚ÄëAugmented Generation). | Training new hires, university courses, or personal skill development. |\n\n---\n\n#### Concrete Examples\n\n| Open‚ÄëSource Model | License | Typical Strengths | Example Projects |\n|-------------------|---------|-------------------|------------------|\n| **LLaMA‚Äë2** (Meta) | Meta‚ÄëLLAMA‚Äë2‚ÄëCommunity (non‚Äëcommercial) / LLaMA‚Äë2‚ÄëChat (commercial) | Strong general‚Äëpurpose performance, good instruction following. | Internal Q&A bots, code‚Äëreview assistants. |\n| **Mistral‚Äë7B** | Apache‚ÄØ2.0 | Efficient 7‚ÄëB model with competitive zero‚Äëshot results. | Low‚Äëlatency inference on a single GPU, edge deployments. |\n| **Phi‚Äë3** (Microsoft) | MIT | Tiny (2.7‚ÄëB) but high‚Äëquality, optimized for CPU/ONNX. | Desktop assistants, mobile apps, or serverless functions. |\n| **Gemma** (Google) | Apache‚ÄØ2.0 | Balanced trade‚Äëoff between size (2‚ÄëB/7‚ÄëB) and instruction following. | Rapid prototyping of chat interfaces. |\n| **OpenChatKit** | MIT | End‚Äëto‚Äëend chat framework with UI, retrieval, and moderation tools. | Building a customer‚Äësupport portal with minimal code. |\n\n---\n\n#### How to Get Started Quickly\n\n1. **Pick a Model & Runtime**  \n   ```bash\n   # Example: pull Mistral‚Äë7B with vLLM (fast serving)\n   pip install vllm\n   vllm serve mistralai/Mistral-7B-Instruct-v0.1 --port 8000\n   ```\n\n2. **Wrap It in an OpenAI‚Äëcompatible API (optional)**  \n   ```bash\n   # Using `vllm`'s built‚Äëin OpenAI‚Äëcompatible endpoint\n   curl http://localhost:8000/v1/chat/completions \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"model\":\"mistral\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain quantum tunneling in 2 sentences\"}]}'\n   ```\n\n3. **Fine‚ÄëTune (if needed)**  \n   - Use **PEFT** (LoRA) for cheap adaptation: `pip install peft transformers accelerate`.  \n   - Example script: `python finetune_lora.py --model mistralai/Mistral-7B-Instruct-v0.1 --train_file data.jsonl`.\n\n4. **Deploy**  \n   - **Docker**: `docker run -p 8000:8000 ghcr.io/vllm/vllm:latest --model mistralai/Mistral-7B-Instruct-v0.1`.  \n   - **Kubernetes**: Helm chart `vllm` or `text-generation-inference` for autoscaling.\n\n---\n\n#### TL;DR\n\nOpen‚Äësource LLMs give developers **control, cost‚Äëefficiency, privacy, and a fast path to innovation**‚Äîall backed by a vibrant community and permissive licensing. They let you turn a generic language model into a **tailored, production‚Äëready service** without being locked into a proprietary API. Whether you‚Äôre building a chatbot, a code‚Äëassistant, or a data‚Äëanalysis pipeline, the open‚Äësource stack provides the building blocks to iterate quickly and own the entire stack."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0trKBWGQHEHq"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 3: Streaming Responses\n",
        "\n",
        "Streaming allows you to receive the LLM's response **token by token** as it's being generated, rather than waiting for the complete response. This is useful for:\n",
        "\n",
        "- **Better UX** - Users see responses appearing in real-time\n",
        "- **Long responses** - Start processing/displaying before completion\n",
        "- **Interactive applications** - Chat interfaces, assistants, etc.\n",
        "\n",
        "## Comparison: `.invoke()` vs `.stream()`\n",
        "\n",
        "| Method | Behavior | Use Case |\n",
        "|--------|----------|----------|\n",
        "| `.invoke()` | Returns complete response at once | Simple Q&A, batch processing |\n",
        "| `.stream()` | Yields response token-by-token | Chat UIs, real-time display |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZOyJpf6HEHr"
      },
      "source": [
        "## Example 5: Regular `.invoke()` - Full Response at Once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8m3knVBHEHr",
        "outputId": "c9a9a08d-b804-4caf-f9ee-837a954b4ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using .invoke() - Full response at once:\n",
            "==================================================\n",
            "**Report on Pros and Cons of Generative AI**\n",
            "\n",
            "**Pros:**\n",
            "- **Creativity Enhancement:** Generative AI can produce original content such as text, images, music, and designs, aiding creative professionals and hobbyists.\n",
            "- **Efficiency and Automation:** It automates content creation tasks, saving time and reducing human effort in areas like writing, coding, and design.\n",
            "- **Personalization:** Enables highly personalized user experiences by generating tailored recommendations, messages, or products.\n",
            "- **Innovation Catalyst:** Facilitates rapid prototyping and idea generation, accelerating innovation in various industries.\n",
            "- **Accessibility:** Helps democratize content creation by providing tools that require less specialized skill or knowledge.\n",
            "\n",
            "**Cons:**\n",
            "- **Quality and Accuracy Issues:** Generated content may contain errors, biases, or misleading information, requiring careful review.\n",
            "- **Ethical Concerns:** Risks include misuse for deepfakes, misinformation, plagiarism, and intellectual property violations.\n",
            "- **Job Displacement:** Automation of creative and repetitive tasks may threaten certain jobs, leading to economic and social challenges.\n",
            "- **Dependence and De-skilling:** Overreliance on AI tools might reduce human skills and critical thinking over time.\n",
            "- **Resource Intensive:** Training and running generative AI models require significant computational power and energy, impacting sustainability.\n",
            "==================================================\n",
            "‚úÖ Complete response received\n"
          ]
        }
      ],
      "source": [
        "# Using ChatOpenAI for this example\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "prompt = \"Write a short report on pros and cons of Generative AI with 3-5 bullets per topic.\"\n",
        "\n",
        "print(\"Using .invoke() - Full response at once:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get full response\n",
        "response = model.invoke(prompt)\n",
        "\n",
        "# Print the complete response\n",
        "print(response.content)\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Complete response received\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "nHnHHNH9V5lD",
        "outputId": "83b74b0b-3da8-4a22-bc6d-c036b7ceab7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Report on Pros and Cons of Generative AI**\n\n**Pros:**\n- **Creativity Enhancement:** Generative AI can produce original content such as text, images, music, and designs, aiding creative professionals and hobbyists.\n- **Efficiency and Automation:** It automates content creation tasks, saving time and reducing human effort in areas like writing, coding, and design.\n- **Personalization:** Enables highly personalized user experiences by generating tailored recommendations, messages, or products.\n- **Innovation Catalyst:** Facilitates rapid prototyping and idea generation, accelerating innovation in various industries.\n- **Accessibility:** Helps democratize content creation by providing tools that require less specialized skill or knowledge.\n\n**Cons:**\n- **Quality and Accuracy Issues:** Generated content may contain errors, biases, or misleading information, requiring careful review.\n- **Ethical Concerns:** Risks include misuse for deepfakes, misinformation, plagiarism, and intellectual property violations.\n- **Job Displacement:** Automation of creative and repetitive tasks may threaten certain jobs, leading to economic and social challenges.\n- **Dependence and De-skilling:** Overreliance on AI tools might reduce human skills and critical thinking over time.\n- **Resource Intensive:** Training and running generative AI models require significant computational power and energy, impacting sustainability."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXuThBssHEHr"
      },
      "source": [
        "## Example 6: Streaming with `.stream()` - Token-by-Token\n",
        "\n",
        "When you call `.stream()`, it returns a **generator** that yields response chunks as they arrive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ajDSM0HEHr",
        "outputId": "159fb9af-611b-4463-d478-4bb654daab3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using .stream() - Token-by-token streaming:\n",
            "==================================================\n",
            "**Report on Pros and Cons of Generative AI**\n",
            "\n",
            "**Pros:**\n",
            "- **Creativity Enhancement:** Generative AI can produce original content such as text, images, music, and code, aiding creative professionals and hobbyists.\n",
            "- **Efficiency and Automation:** It automates repetitive tasks like content creation, data synthesis, and design, saving time and reducing human effort.\n",
            "- **Personalization:** Enables highly personalized user experiences by generating tailored recommendations, responses, and products.\n",
            "- **Innovation Catalyst:** Facilitates rapid prototyping and idea generation, accelerating innovation across industries.\n",
            "- **Accessibility:** Helps democratize content creation by providing tools that require less specialized skill or knowledge.\n",
            "\n",
            "**Cons:**\n",
            "- **Quality and Accuracy Issues:** Generated content may contain errors, biases, or misleading information, requiring careful review.\n",
            "- **Ethical Concerns:** Risks include misuse for deepfakes, misinformation, plagiarism, and intellectual property violations.\n",
            "- **Job Displacement:** Automation of creative and routine tasks may threaten certain jobs, leading to economic and social challenges.\n",
            "- **Dependence and De-skilling:** Overreliance on AI tools might reduce human skills and critical thinking over time.\n",
            "- **Resource Intensive:** Training and running generative models demand significant computational power and energy, impacting sustainability.\n",
            "==================================================\n",
            "‚úÖ Streaming complete\n"
          ]
        }
      ],
      "source": [
        "# Same model and prompt as above\n",
        "prompt = \"Write a short report on pros and cons of Generative AI with 3-5 bullets per topic.\"\n",
        "\n",
        "print(\"Using .stream() - Token-by-token streaming:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "full_response = \"\"\n",
        "# Stream the response\n",
        "# Each chunk contains a small piece of the response\n",
        "for chunk in model.stream(prompt):\n",
        "    # Print each chunk as it arrives (without newline)\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "    full_response += chunk.content\n",
        "\n",
        "print()  # Add newline at the end\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Streaming complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(full_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "-KZUi_B9WbIJ",
        "outputId": "302d3105-06e1-4943-9042-6039b8d082fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Report on Pros and Cons of Generative AI**\n\n**Pros:**\n- **Creativity Enhancement:** Generative AI can produce original content such as text, images, music, and code, aiding creative professionals and hobbyists.\n- **Efficiency and Automation:** It automates repetitive tasks like content creation, data synthesis, and design, saving time and reducing human effort.\n- **Personalization:** Enables highly personalized user experiences by generating tailored recommendations, responses, and products.\n- **Innovation Catalyst:** Facilitates rapid prototyping and idea generation, accelerating innovation across industries.\n- **Accessibility:** Helps democratize content creation by providing tools that require less specialized skill or knowledge.\n\n**Cons:**\n- **Quality and Accuracy Issues:** Generated content may contain errors, biases, or misleading information, requiring careful review.\n- **Ethical Concerns:** Risks include misuse for deepfakes, misinformation, plagiarism, and intellectual property violations.\n- **Job Displacement:** Automation of creative and routine tasks may threaten certain jobs, leading to economic and social challenges.\n- **Dependence and De-skilling:** Overreliance on AI tools might reduce human skills and critical thinking over time.\n- **Resource Intensive:** Training and running generative models demand significant computational power and energy, impacting sustainability."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, clear_output, HTML\n",
        "\n",
        "prompt = \"Write a short report on pros and cons of Generative AI with 3-5 bullets per topic.\"\n",
        "\n",
        "full_response = \"\"\n",
        "markdown_display = display(Markdown(\"*Generating response...*\"), display_id=True)\n",
        "\n",
        "for chunk in model.stream(prompt):\n",
        "    full_response += chunk.content\n",
        "    # Update the same display object\n",
        "    markdown_display.update(Markdown(full_response + \" ‚ñå\"))  # ‚ñå is a cursor effect\n",
        "\n",
        "# Remove cursor at the end\n",
        "markdown_display.update(Markdown(full_response))\n",
        "print(\"‚úÖ Streaming complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "5PgxX1jCWKc_",
        "outputId": "e638604c-dd41-4224-cef0-3d0194974daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Report on Pros and Cons of Generative AI**\n\n**Pros:**\n- **Creativity Enhancement:** Generative AI can produce original content such as text, images, music, and designs, aiding creative professionals and hobbyists.\n- **Efficiency and Automation:** It automates content creation, reducing time and effort required for tasks like writing, coding, and graphic design.\n- **Personalization:** Enables highly personalized user experiences by generating tailored recommendations, messages, or products.\n- **Innovation Support:** Facilitates rapid prototyping and idea generation in various fields including marketing, entertainment, and research.\n- **Accessibility:** Helps individuals with disabilities by generating assistive content, such as text-to-speech or image descriptions.\n\n**Cons:**\n- **Quality and Accuracy Issues:** Generated content may contain errors, biases, or misleading information, requiring human oversight.\n- **Ethical Concerns:** Risks include misuse for deepfakes, misinformation, plagiarism, and intellectual property violations.\n- **Job Displacement:** Automation of creative and routine tasks may threaten certain jobs, leading to economic and social challenges.\n- **Dependence and De-skilling:** Overreliance on AI tools might reduce human skills and critical thinking over time.\n- **Resource Intensive:** Training and running generative AI models require significant computational power and energy, impacting sustainability."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Streaming complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(full_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "LjMFExF6XSHG",
        "outputId": "0a9700c7-5a1f-43e3-916b-b0debacd6821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Report on Pros and Cons of Generative AI**\n\n**Pros:**\n- **Creativity Enhancement:** Generative AI can produce original content such as text, images, music, and designs, aiding creative professionals and hobbyists.\n- **Efficiency and Automation:** It automates content creation, reducing time and effort required for tasks like writing, coding, and graphic design.\n- **Personalization:** Enables highly personalized user experiences by generating tailored recommendations, messages, or products.\n- **Innovation Support:** Facilitates rapid prototyping and idea generation in various fields including marketing, entertainment, and research.\n- **Accessibility:** Helps individuals with disabilities by generating assistive content, such as text-to-speech or image descriptions.\n\n**Cons:**\n- **Quality and Accuracy Issues:** Generated content may contain errors, biases, or misleading information, requiring human oversight.\n- **Ethical Concerns:** Risks include misuse for deepfakes, misinformation, plagiarism, and intellectual property violations.\n- **Job Displacement:** Automation of creative and routine tasks may threaten certain jobs, leading to economic and social challenges.\n- **Dependence and De-skilling:** Overreliance on AI tools might reduce human skills and critical thinking over time.\n- **Resource Intensive:** Training and running generative AI models require significant computational power and energy, impacting sustainability."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az_FK-2sHEHs"
      },
      "source": [
        "### Understanding the Streaming Output\n",
        "\n",
        "In the streaming example above:\n",
        "\n",
        "- `model.stream(prompt)` returns a **generator**\n",
        "- Each iteration yields a **chunk** (small piece of the response)\n",
        "- `chunk.content` contains the text for that chunk\n",
        "- `end=\"\"` prevents Python from adding newlines after each chunk\n",
        "- `flush=True` ensures immediate display of each chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BD6O5E-HEHt"
      },
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "### 1. **Two Ways to Initialize Models**\n",
        "\n",
        "```python\n",
        "# Method 1: init_chat_model (flexible, universal)\n",
        "from langchain.chat_models import init_chat_model\n",
        "model = init_chat_model(\"gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Method 2: Provider-specific (more control)\n",
        "from langchain_openai import ChatOpenAI\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "```\n",
        "\n",
        "### 2. **Simple Prompting**\n",
        "\n",
        "```python\n",
        "# Get full response\n",
        "response = model.invoke(\"Your prompt here\")\n",
        "print(response.content)\n",
        "```\n",
        "\n",
        "### 3. **Streaming Responses**\n",
        "\n",
        "```python\n",
        "# Stream token-by-token\n",
        "for chunk in model.stream(\"Your prompt here\"):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Now that you understand LLM connections, you can explore:\n",
        "\n",
        "- **Prompt Templates** - Create reusable, parameterized prompts\n",
        "- **Tool Use** - Give LLMs access to external tools/APIs\n",
        "- **RAG (Retrieval Augmented Generation)** - Connect LLMs to your data\n",
        "- **Agents** - Build autonomous AI systems\n",
        "\n",
        "Happy building! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}